{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pommerman\n",
    "\n",
    "ボンバーマンの学習を行う\n",
    "\n",
    "![pommerman](https://www.pommerman.com/static/media/pommerman.abbcd943.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前回コンペの対戦などが見れる\n",
    "\n",
    "https://www.pommerman.com/leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今年のルール\n",
    "\n",
    "free-for-all, teamに加えてエージェント間の通信が可能な `PommeRadioCompetition-v2` が追加された\n",
    "\n",
    "https://www.pommerman.com/competitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今までの環境との違い\n",
    "\n",
    "- 状態が爆発的に多い\n",
    "- 対環境で自分の行動だけが環境に影響があって予測しやすいが、pommermanはチームや相手が複数いる。\n",
    "- 相手も増えて難しくなるが、仲間と連携も可能だったり奥深さも増す"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用意されたエージェント\n",
    "\n",
    "- [RandomAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/random_agent.py) ランダムに動くエージェント (すぐ自爆するので弱い)\n",
    "- [SimpleAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/simple_agent.py) 人間が考えたロジックで動作するエージェント(かなり強い)\n",
    "- [PlayerAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/player_agent.py) 人間が操作可能なエージェント。キーボード上下左右とスペースで操作可能\n",
    "- 以下DockerAgentとHttpAgentを利用してサーバーを立てて対戦が可能\n",
    "  - [HttpAgent](https://github.com/MultiAgentLearning/playground/blob/master/pommerman/agents/http_agent.py)\n",
    "  - DockerAgent [Dockerfile](https://github.com/MultiAgentLearning/playground/blob/master/examples/docker-agent/Dockerfile) [simple_ffa_run.py](https://github.com/MultiAgentLearning/playground/blob/master/examples/simple_ffa_run.py#L20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## エージェントの自作\n",
    "\n",
    "シンプルな停止エージェント。\n",
    "\n",
    "```python\n",
    "from pommerman.agents import BaseAgent\n",
    "\n",
    "class StoppingAgent(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        # 0 stop, 1 up, 2 down, 3 left, 4 right, 5 bom\n",
    "        return 0\n",
    "```\n",
    "\n",
    "obsにはボードの状態やアイテム取得状況など必要な情報が全て入っている。\n",
    "\n",
    "アクションの種類やボードの数値については以下ドキュメントに書かれている。\n",
    "https://github.com/MultiAgentLearning/playground/tree/master/pommerman"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各エージェントの動作とシンプルな実行方法\n",
    "\n",
    "`pommerman_simple_agents.py` に実装してます。\n",
    "\n",
    "jupyter notebook上でrenderする方法が探せなかったのでlocalで実行します。\n",
    "\n",
    "```python\n",
    "env = pommerman.make('PommeFFACompetition-v0', [\n",
    "    agents.PlayerAgent(),\n",
    "    agents.SimpleAgent(),\n",
    "    agents.RandomAgent(),\n",
    "    StoppingAgent(),\n",
    "])\n",
    "\n",
    "for i_episode in range(1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        env.render()\n",
    "        actions = env.act(state)\n",
    "        state, reward, done, info = env.step(actions)\n",
    "        if done:\n",
    "            win_player = info['winners'][0] + 1\n",
    "            print(f'win {win_player}P')\n",
    "    print(f'Episode {i_episode} finished')\n",
    "env.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習で実装するエージェント\n",
    "\n",
    "重要な要素\n",
    "\n",
    "- `Rewards.get_rewards` のリワード設計\n",
    "- `EnvWrapper.featurize` の学習すべき状態設計\n",
    "- `create_model` モデル設計\n",
    "- `create_dqn` DQNパラメータ調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "from pommerman.agents import BaseAgent, SimpleAgent\n",
    "from pommerman.configs import ffa_v0_fast_env\n",
    "from pommerman.envs.v0 import Pomme\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.core import Env, Processor\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import MaxBoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rewards:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def get_rewards(self, obs_now, action_now, reward):\n",
    "\n",
    "        # TODO: リワードの調整\n",
    "\n",
    "        # ボードの詳細\n",
    "        # https://github.com/MultiAgentLearning/playground/tree/master/pommerman\n",
    "\n",
    "        # 例: 爆弾獲得\n",
    "        if self.ammo < obs_now['ammo']:\n",
    "            reward += 0.2\n",
    "        self.ammo = obs_now['ammo']\n",
    "\n",
    "        # 例: 火力獲得\n",
    "        if self.blast_strength < obs_now['blast_strength']:\n",
    "            reward += 0.2\n",
    "        self.blast_strength = obs_now['blast_strength']\n",
    "\n",
    "        # 例: キック獲得\n",
    "        if self.can_kick is False and obs_now['can_kick'] is True:\n",
    "            reward += 0.2\n",
    "        self.can_kick = obs_now['can_kick']\n",
    "\n",
    "        # 例: 生存step数ペナルティ\n",
    "        reward += obs_now['step_count'] * -0.0001\n",
    "\n",
    "        # print(reward)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def reset(self):\n",
    "        self.ammo = 1\n",
    "        self.blast_strength = 2\n",
    "        self.can_kick = False\n",
    "\n",
    "        # print(f'Rewards.reset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvWrapper(Env):\n",
    "    def __init__(self, gym):\n",
    "        self.gym = gym\n",
    "        self.rewardShaping = Rewards()\n",
    "\n",
    "    def __del__(self):\n",
    "        self.close()\n",
    "\n",
    "    def __str__(self):\n",
    "        return '<{} instance>'.format(type(self).__name__)\n",
    "\n",
    "    def render(self, mode='human', close=False):\n",
    "        self.gym.render(mode=mode, close=close)\n",
    "\n",
    "    def close(self):\n",
    "        self.gym.close()\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        raise self.gym.seed(seed)\n",
    "\n",
    "    def configure(self, *args, **kwargs):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def reset(self):\n",
    "        # print('EnvWrapper.reset')\n",
    "        self.rewardShaping.reset()\n",
    "        obs = self.gym.reset()\n",
    "        agent_obs = self.featurize(obs[self.gym.training_agent])\n",
    "        return agent_obs\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(f'EnvWrapper.step action = {action}')\n",
    "        obs = self.gym.get_observations()\n",
    "        all_actions = self.gym.act(obs)\n",
    "        all_actions.insert(self.gym.training_agent, action)\n",
    "        state, reward, terminal, info = self.gym.step(all_actions)\n",
    "        action = all_actions[self.gym.training_agent]\n",
    "        agent_state = self.featurize(state[self.gym.training_agent])\n",
    "        agent_reward = reward[self.gym.training_agent]\n",
    "\n",
    "        agent_reward = self.rewardShaping.get_rewards(\n",
    "            obs[self.gym.training_agent],\n",
    "            action,\n",
    "            agent_reward,\n",
    "        )\n",
    "\n",
    "        return agent_state, agent_reward, terminal, info\n",
    "\n",
    "    def featurize(self, obs):\n",
    "\n",
    "        # TODO: トレーニングエージェントのobsを加工して返す\n",
    "\n",
    "        # 例: 自分は10、敵は11に変更 (11, 11, 12)\n",
    "        # board = obs['board']\n",
    "        # board = np.where(13 == board, 10, board)\n",
    "        # board = np.where(10 < board, 11, board)\n",
    "\n",
    "        # 例: カテゴリカルデータに変更\n",
    "        # from tensorflow.keras.utils import to_categorical\n",
    "        # nb_classes = 12\n",
    "        # board = to_categorical(board, nb_classes).astype(np.float32)\n",
    "        # print(board.shape)\n",
    "\n",
    "        return obs['board']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomProcessor(Processor):\n",
    "    def process_state_batch(self, batch):\n",
    "        # print(f'CustomProcessor.process_state_batch batch = {batch.shape}')\n",
    "        return batch\n",
    "\n",
    "    def process_info(self, info):\n",
    "        info['result'] = info['result'].value\n",
    "        return info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(BaseAgent):\n",
    "    def act(self, obs, action_space):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env():\n",
    "    config = ffa_v0_fast_env()\n",
    "    env = Pomme(**config[\"env_kwargs\"])\n",
    "    # env.seed(0)\n",
    "\n",
    "    agent_id = 0\n",
    "\n",
    "    agents = [\n",
    "        DQN(config[\"agent\"](0, config[\"game_type\"])),\n",
    "        SimpleAgent(config[\"agent\"](1, config[\"game_type\"])),\n",
    "        SimpleAgent(config[\"agent\"](2, config[\"game_type\"])),\n",
    "        SimpleAgent(config[\"agent\"](3, config[\"game_type\"])),\n",
    "    ]\n",
    "\n",
    "    env.set_agents(agents)\n",
    "\n",
    "    env.set_training_agent(agents[agent_id].agent_id)\n",
    "    env.set_init_game_state(None)\n",
    "\n",
    "    return env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape, output_units, history_length):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(\n",
    "        32,\n",
    "        kernel_size=2,\n",
    "        strides=(1, 1),\n",
    "        input_shape=(history_length, input_shape[0], input_shape[1]),\n",
    "        activation='relu',\n",
    "    ))\n",
    "    model.add(Conv2D(\n",
    "        64,\n",
    "        kernel_size=2,\n",
    "        strides=(1, 1),\n",
    "        activation='relu'),\n",
    "    )\n",
    "    model.add(Conv2D(\n",
    "        64,\n",
    "        kernel_size=2,\n",
    "        strides=(1, 1),\n",
    "        activation='relu',\n",
    "    ))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(\n",
    "        units=128,\n",
    "        activation='relu',\n",
    "    ))\n",
    "    model.add(Dense(\n",
    "        units=128,\n",
    "        activation='relu',\n",
    "    ))\n",
    "    model.add(Dense(\n",
    "        units=output_units,\n",
    "        activation='linear',\n",
    "    ))\n",
    "\n",
    "    # print(model.input_shape)\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dqn(model, history_length):\n",
    "    memory = SequentialMemory(limit=500000, window_length=history_length)\n",
    "    policy = MaxBoltzmannQPolicy()\n",
    "\n",
    "    dqn = DQNAgent(\n",
    "        model=model,\n",
    "        nb_actions=model.output_shape[1],\n",
    "        memory=memory,\n",
    "        policy=policy,\n",
    "        processor=CustomProcessor(),\n",
    "        nb_steps_warmup=512,\n",
    "        enable_dueling_network=True,\n",
    "        dueling_type='avg',\n",
    "        target_model_update=5e2,\n",
    "        batch_size=32,\n",
    "    )\n",
    "    dqn.compile(Adam(lr=1e-3), metrics=['mae'])\n",
    "\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = 'models/pommerman/keras_weights.h5'\n",
    "history_length = 4\n",
    "input_shape = (11, 11)\n",
    "output_units = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = get_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_wrapper = EnvWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 3, 10, 32)         1440      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 2, 9, 64)          8256      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 1, 8, 64)          16448     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               65664     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 109,094\n",
      "Trainable params: 109,094\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(input_shape, output_units, history_length)\n",
    "dqn = create_dqn(model, history_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(weight_path):\n",
    "    dqn.load_weights(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 2000000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "10000/10000 [==============================] - 171s 17ms/step - reward: -0.0372\n",
      "400 episodes - episode_reward: -0.931 [-1.365, -0.315] - loss: 0.014 - mae: 0.404 - mean_q: 0.524 - result: 2.949\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 205s 21ms/step - reward: -0.0303\n",
      "329 episodes - episode_reward: -0.920 [-1.263, -0.108] - loss: 0.011 - mae: 0.216 - mean_q: 0.235 - result: 2.960\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      " 2831/10000 [=======>......................] - ETA: 2:31 - reward: -0.0336done, took 436.535 seconds\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dqn.fit(\n",
    "        env_wrapper,\n",
    "        nb_steps=2000000,  # 8h\n",
    "        visualize=True,\n",
    "        nb_max_episode_steps=env._max_steps,\n",
    "    )\n",
    "except KeyboardInterrupt:\n",
    "    pass\n",
    "finally:\n",
    "    dqn.save_weights(weight_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 結果\n",
    "\n",
    "CPUで数時間程度では全然学習できない。\n",
    "\n",
    "爆弾を置くと自分で自爆するので置かない方が安全だと気づいてしまう。\n",
    "\n",
    "動かないplayerと対戦させたり、壁壊すことに報酬与えたり、爆弾置けるのに置かないと罰を与えたりしたが若干爆風を避けたかと思う程度で全然SimpleAgentに勝てるレベルでは学習できなかった。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 考察\n",
    "\n",
    "1. トレーニングエージェントのobsを加工して返す `EnvWrapper.featurize`\n",
    "  - obsにはボードの状態やアイテムの取得状況など必要な状態は全て揃っているので、そのデータを加工してネットワークのinput_shapeを変更する\n",
    "  - 全部の情報を利用すると情報過剰で収束にかなり時間がかかる。最初は周りの9マスのみに変更するなど情報量を削って試してみる\n",
    "  - 1と2は連続値ではなくidなのでカテゴリカルデータに変更する\n",
    "    - [[1, 2], [3, 4]] => [[[1,0,0,0]], [0,1,0,0]], [[0,0,1,0], [0,0,0,1]]]\n",
    "  - 自分(10)、敵(11,12,13)なので自分以外は敵としてデータを調整するなど\n",
    "2. リワードの調整を行う `Rewards.get_rewards`\n",
    "  - 最初に敵倒せるなど不可能なのでよく起こりうることに対して報酬を与えれないか考える\n",
    "    - 壁壊したら報酬与える\n",
    "    - アイテムとったら報酬与える\n",
    "  - ボム置かなくなる対策\n",
    "    - ボムを置いたら報酬を与える\n",
    "    - 時間経過(step)ごとにマイナス報酬を増大/減少させる\n",
    "      - 生存のために減少させると動かなくなりうる\n",
    "      - 挑戦させるためにマイナス報酬を増大させる\n",
    "3. パラメータ調整やネットワークを変えてみる\n",
    "  - DQNのパラメータ、policyや割引率など変えてみる\n",
    "  - DQN以外にもNAFAgent,DDPGAgent,SARSAAgent,CEMAgent,PPOAgentなどいっぱいある\n",
    "  - 教師あり学習でも良いしニューラルネットワークでなくても良い。別フレームワークでも良い\n",
    "4. その他のアイディアを考える\n",
    "  - SimpleAgentには全く勝てないのでStoppingAgentを実装して動かない相手と対戦して知識をつける\n",
    "  - BaseLineAgentの動作を教師あり学習させたあと、蒸留してネットワークを縮める\n",
    "  - ActionFiterのような知識(半ロジック)を埋め込む\n",
    "  - PlayerAgent(キーボードで人間が操作可能)なエージェントのログを教師あり学習で学習する\n",
    "  - 学習にかなり時間かかるのでtry and error難しい。GPUなど高火力で攻める"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## コンペランキングと実装から考察\n",
    "\n",
    "https://www.pommerman.com/leaderboard\n",
    "https://challonge.com/ja/runningPommermanNeurips\n",
    "\n",
    "- ffa 1位 [YichenGong/Agent47Agent](https://github.com/YichenGong/Agent47Agent)\n",
    "- team 1位, 2~3位\n",
    "  - [記事](https://www.ibm.com/blogs/think/jp-ja/eal-time-sequential-decision-making/)\n",
    "  - [スライド](https://www.slideshare.net/TakayukiOsogami/pommerman?next_slideshow=1)\n",
    "  - [Docker hub](https://hub.docker.com/layers/multiagentlearning/hakozakijunctions/latest/images/sha256-1ef23f0e35a6a404e7b4cb7a4356ff8d10748f1fd7f8d9fef2e76fb4eda8ce41)\n",
    "  - [実装](https://github.com/takahasixxx/GGG)\n",
    "  - [実装 アクション決定ロジック](https://github.com/takahasixxx/GGG/blob/c88fac39964ce74ff0084d37a7b00937be6088e9/src/com/ibm/trl/BBM/mains/ActionEvaluator.java#L42)\n",
    "- 学習エージェント2位 skynet\n",
    "  - [記事](https://www.borealisai.com/en/blog/pommerman-team-competition-or-how-we-learned-stop-worrying-and-love-battle/)\n",
    "  - [ActionFilterの実装](https://github.com/BorealisAI/pommerman-baseline)\n",
    "  - [資料](https://www.researchgate.net/publication/332897569_Skynet_A_Top_Deep_RL_Agent_in_the_Inaugural_Pommerman_Team_Competition)\n",
    "  - [論文](https://arxiv.org/abs/1907.11788) [論文](https://arxiv.org/abs/1905.01360)\n",
    "  - Borealis AI team 学習エージェントカテゴリで2位 (非学習)ヒューリスティックエージェントを含むグローバルランキングで5位になりました。\n",
    "  - 3番目のブロックであるActionFilterモジュールは、エージェントに事前知識をインストールするという哲学に基づいて、エージェントにすべきでないことを伝えることで構築され、エージェントが試行錯誤、つまり学習によって何をすべきかを発見できるようにしました。\n",
    "\n",
    "\n",
    "\n",
    "状態が多すぎるゲームにおいては完全に深層学習だけでは(現実的ではない学習量になって)実装できず、ある程度ロジックによる補助が必要そう。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 参考\n",
    "\n",
    "keras-rl実装\n",
    "- https://github.com/kenkangg/Multi-Agent-Cooperative-Competitive-Environments\n",
    "- https://github.com/Borzyszkowski/RL-Bomberman-Gradient\n",
    "\n",
    "tensorforce実装\n",
    "- https://github.com/MultiAgentLearning/playground/blob/master/notebooks/Playground.ipynb\n",
    "\n",
    "各学習結果\n",
    "- https://github.com/papkov/pommerman-x/blob/master/DQN.ipynb\n",
    "- https://github.com/tambetm/pommerman-baselines/tree/master/imitation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
