{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 機械学習基礎\n",
    "\n",
    "### 用途の分類\n",
    "\n",
    "![用途の分類](https://products.sint.co.jp/hs-fs/hubfs/images/aisia/blog/9_001.png?width=1556&height=1060&name=9_001.png)\n",
    "\n",
    "![用途](https://cdn-xtrend.nikkei.com/atcl/contents/18/00076/00002/01.png?__scale=w:640,h:500&_sh=086000490b)\n",
    "\n",
    "### 機械学習(マシンラーニング)/ニューラルネットワーク/ディープラーニング\n",
    "\n",
    "![マシンラーニング/ニューラルネットワーク/ディープラーニング](http://www.dri.co.jp/dri_forum/wp-content/uploads/2018/08/5145-01.bmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習基礎\n",
    "\n",
    "https://www.slideshare.net/takahirokubo7792/techcircle-18-python-openai-gym?ref=https://techcircle.connpass.com/event/43844/presentation/\n",
    "\n",
    "### OpenAiGym\n",
    "\n",
    "- Env(環境)から状態をもらう\n",
    "- 状態をAgent(プレイヤー)に渡してAction(行動)をもらう\n",
    "- Action(行動)をEnv(環境)に渡して報酬をもらいつつ、ゲームを進める\n",
    "- 報酬からAgent(プレイヤー)は報酬が最終的に最大になるように学習する\n",
    "\n",
    "### なぜ強化学習か\n",
    "\n",
    "教師あり学習でも良いのではないか。\n",
    "\n",
    "データセットに含まれる状態とアクションが本当に最大の報酬にたどりつく為のデータになっているか。\n",
    "\n",
    "将棋などはプロの棋譜から教師あり学習可能だが、そのプロ以上の能力を持てない。(必ずしも正解とは限らない)\n",
    "\n",
    "強化学習で自ら学習する必要がある。\n",
    "\n",
    "### マルコフ決定過程\n",
    "\n",
    "ポリシー(戦略)にしたがって行動を決定し、報酬をもらう\n",
    "\n",
    "もらえる報酬が最大になる行動はどのようなものか探索する\n",
    "\n",
    "### ベルマン方程式\n",
    "\n",
    "リスクとリターンを考慮して報酬の期待値を計算する\n",
    "\n",
    "### Qラーニング\n",
    "\n",
    "これまでの行動評価と今の行動評価をどのくらい信じるか計算する\n",
    "\n",
    "### グリーディ法\n",
    "\n",
    "初期のランダムに決まったQ値がたまたま大きな値となった行動だけが常に選択されてしまう可能性がある\n",
    "\n",
    "一定割合でランダムに行動を選択させたり(探索)、Q値の大きい行動を選択させたり(活用)する\n",
    "\n",
    "### DQN (Deep Q Network)\n",
    "\n",
    "ニューラルネットワークを利用したQラーニング\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今回使うライブラリ/フレームワーク\n",
    "\n",
    "- open ai gym\n",
    "  - 強化学習の研究などに利用できるゲームを提供している\n",
    "- keras\n",
    "  - ニューラルネットワーク実装の高レベルライブラリ\n",
    "  - tensorflowのwrapper、tensorflow2からkerasと密接に連携でき、公式のチュートリアルもkerasに変わっている\n",
    "  - tensorboradやtensorflow hub, tensorflow servingなども(一部使えなかったりするが)ちゃんとサポートされるはず\n",
    "- keras-rl\n",
    "  - kerasのネットワーク設計のしやすさを生かした強化学習用ライブラリ。\n",
    "\n",
    "他にも強化学習フレームワークとしてtensorforceがある。\n",
    "pommermanのexampleはtensorforceだったので `tensorforce_*.py` に実装してますが使いやすいもののドキュメントが厳しいので価値は低いかも。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基本的な実装イメージ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import gym\n",
    "\n",
    "# 環境作成と初期化\n",
    "env = gym.make('CartPole-v0')\n",
    "state = env.reset()\n",
    "\n",
    "# エージェント作成\n",
    "agent = create_agent()\n",
    "\n",
    "for _ in range(1000):\n",
    "    # 描画\n",
    "    env.render()\n",
    "    \n",
    "    # エージェントに状態を与えてアクションを決定させる\n",
    "    action = agent.action(state)\n",
    "    \n",
    "    # アクションを環境に渡してゲームを進める\n",
    "    observation, reward = env.step(action)\n",
    "    \n",
    "    # 今の行動を評価する\n",
    "    agent.observe(reward)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今日実装するもの\n",
    "\n",
    "cartpole\n",
    "\n",
    "![cartpole](https://book.mynavi.jp/files/user/manatee/rensai/019_kyoukagakusyuu/09/cartpole.gif)\n",
    "\n",
    "breakout\n",
    "\n",
    "![breakout](https://thumbs.gfycat.com/AnchoredScornfulAustraliansilkyterrier-size_restricted.gif)\n",
    "\n",
    "pommerman\n",
    "\n",
    "![pommerman](https://www.pommerman.com/static/media/pommerman.abbcd943.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
